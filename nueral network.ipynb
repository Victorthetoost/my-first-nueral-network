{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cd931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#CODE TO BE TRAINED ON IMAGES. 24X24 PIXELS\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import copy\n",
    "nnfs.init()\n",
    "\n",
    "Neurons_per_Layer = 64\n",
    "\n",
    "# Dense layer\n",
    "class Layers_Dense:\n",
    "\n",
    "    #initializes the weights and biases\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) #initializes random weights for teh layer, .01 to keep them small\n",
    "\n",
    "        self.bias = np.zeros((1, n_neurons)) # initializes the biases to 0\n",
    "        \n",
    "    # does the dot product of the inputs and weights, and adds the bias to the output.\n",
    "    # this does the inital prediction which is then used to calculate loss and what needs to be changed in the weights and biases.\n",
    "    def forward(self, x):\n",
    "        self.inputs = x #stores the inputs for the backwards pass (optimization step)\n",
    "\n",
    "        self.output = np.dot(x, self.weights) + self.bias #calculates teh dot product for the next layer, and adds the bias to the output.\n",
    "\n",
    "    # this is the backpropagation step, which gets the gradients of the weights, biases, and inputs.\n",
    "    # this is used to update the weights and biases in the optimizer step.\n",
    "    def backward(self, dvalues):\n",
    "        #basically it helps show what happened to the inputs during the forward pass, and how they affect the output.\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    def forward(self, x):\n",
    "        self.inputs = x\n",
    "        self.output = np.maximum(0, x)\n",
    "        # applies the ReLU activation function, which sets all negative values to zero. and all the positive values remain unchanged.\n",
    "        #thisd gives more control over the neurons which increases accuracy over a linear function (helps with nonlinear data and solutions)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        # during backpropagation, it sets the gradients to zero for inputs that were negative (since ReLU outputs zero for those inputs).\n",
    "        #this ensures that the gradients are only propagated back through neurons that were active (i.e., had positive inputs).\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True)) # subtracts the max value for numerical stability\n",
    "        # this prevents overflow when calculating the exponential of large values.\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True) # gets the probabilities by dividing the exponentials by their sum\n",
    "        # stores the probabilities in the output attribute for later use (e.g., during loss calculation).\n",
    "        # This ensures that the output probabilities sum to 1 for each sample, making it suitable\n",
    "        self.output = probabilities\n",
    "        # applies the softmax activation function, which converts raw scores (logits) into probabilities that sum to 1 for each sample\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    # This method calculates the loss by first computing the sample losses using the forward method and then averaging them to get the overall data loss.\n",
    "    #this is useful during the training process to see if the changes are positive or negative. \n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred) # gets the number of samples in the predictions\n",
    "        # This is used to calculate the loss for each sample in the batch.\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # subtracts the max value for numerical stability\n",
    "        # This prevents overflow when calculating the exponential of large values.\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    # This method calculates the categorical cross-entropy loss between the predicted probabilities (y_pred) and the true labels (y_true).\n",
    "    # basically it uses the log function to exxagerate the loss when its wrong, and maintain a reasonable answer when its right. \n",
    "    # so the difference between 70& wrong and 50% wrong is a lot more than 50% wrong ant 30% wrong.\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        # This initializes the Adam optimizer with parameters for learning rate, momentum coefficients (beta_1 and beta_2), \n",
    "        # and a small constant (epsilon) to prevent division by zero.\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #initializes the memomentum and cache for weights and biases if they don't exist\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.bias)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Corrected momentums\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                             (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                           (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Parameter update\n",
    "        layer.weights -= self.learning_rate * weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.bias -= self.learning_rate * bias_momentums_corrected / \\\n",
    "                      (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        pass\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "data = pd.read_csv('fuckass neural network/mnist_train.csv')\n",
    "# Create dataset X is the data, y is the labels\n",
    "X = data.iloc[:, 1:].values / 255.0  # Normalize pixel values 0-255\n",
    "# X is the data, which is the pixel values of the images normalized to a range of 0 to 1.\n",
    "# Each row in X represents a flattened image, so a 28x28 pixel image becomes\n",
    "y = data.iloc[:, 0].values  # number labels, 0-9\n",
    "\n",
    "# Create model\n",
    "#the first layer is (number of inputs, number of neurons in the layer)\n",
    "dense1 = Layers_Dense(784, Neurons_per_Layer)#  # 784 inputs for a 28x28 pixel image flattened into a single row\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#the second layer is (previous layer's neurons, number of neurons in the layer)\n",
    "dense_hidden = Layers_Dense(Neurons_per_Layer, Neurons_per_Layer) #layer inbetween the first and second\n",
    "activation_hidden = Activation_ReLU()     # ReLU activation\n",
    "\n",
    "#the third layer is (previous layer's neurons, number of outputs)\n",
    "dense2 = Layers_Dense(Neurons_per_Layer, 10)  # 10 outputs for digits 0-9\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_weights = {}\n",
    "\n",
    "for epoch in range(2001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense_hidden.forward(activation1.output)\n",
    "    activation_hidden.forward(dense_hidden.output)\n",
    "\n",
    "    dense2.forward(activation_hidden.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # Save best weights if accuracy improves\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_weights = {\n",
    "            'dense1': copy.deepcopy(dense1),\n",
    "            'dense_hidden': copy.deepcopy(dense_hidden),\n",
    "            'dense2': copy.deepcopy(dense2)\n",
    "        }\n",
    "\n",
    "    # Backward pass\n",
    "    dvalues = activation2.output.copy()\n",
    "    dvalues[range(len(y)), y] -= 1\n",
    "    dvalues = dvalues / len(y)\n",
    "\n",
    "    dense2.backward(dvalues)\n",
    "    activation_hidden.backward(dense2.dinputs)\n",
    "    dense_hidden.backward(activation_hidden.dinputs)\n",
    "    activation1.backward(dense_hidden.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense_hidden)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    # Print results\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# After training, restore the best model if needed:\n",
    "dense1 = best_weights['dense1']\n",
    "dense_hidden = best_weights['dense_hidden']\n",
    "dense2 = best_weights['dense2']\n",
    "\n",
    "print(f\"\\nBest Accuracy Achieved: {best_accuracy:.4f} (model restored)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
